# This file was automatically generated by Cerebrium as a starting point for your project. 
# You can edit it as you wish.
# If you would like to learn more about your Cerebrium config, please visit https://docs.cerebrium.ai/cerebrium/environments/config-files#config-file-example

[cerebrium.build]
predict_data = "{\"prompt\": \"Here is some example predict data for your config.yaml which will be used to test your predict function on build.\"}"
force_rebuild = false
disable_animation = false
log_level = "INFO"
disable_confirmation = false
shell_commands = [
    "apt-get update && apt-get -y install openmpi-bin libopenmpi-dev",
    "pip3 install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com",
    "rm -rf TensorRT-LLM",
    "git clone https://github.com/NVIDIA/TensorRT-LLM.git",
    "cd TensorRT-LLM/examples/gemma",
    "pip3 install -r requirements.txt",
    "CKPT_PATH=./7b-it",
    "UNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_2b_it_tensorrt_llm/bf16/tp1/",
    "ENGINE_PATH=/tmp/gemma/2B/bf16/1-gpu/",
    "python3 convert_checkpoint.py  --ckpt-type torch --model-dir ${CKPT_PATH} --dtype bfloat16 --world-size 1 --output-model-dir ${UNIFIED_CKPT_PATH}",
    "trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} --gemm_plugin bfloat16 --gpt_attention_plugin bfloat16 --max_batch_size 8 --max_input_len 3000 --max_output_len 100 --context_fmha enable --output_dir ${ENGINE_PATH}"
]



[cerebrium.deployment]
name = "tensor-rt"
python_version = "3.10"
include = "[./*, main.py]"
exclude = "[./.*, ./__*]"
cuda_version = "12"

[cerebrium.hardware]
gpu = "AMPERE_A5000"
cpu = 2
memory = 16.0
gpu_count = 1

[cerebrium.scaling]
min_replicas = 0
max_replicas = 5
cooldown = 60

[cerebrium.dependencies.pip]
torch = ">=2.0.0"
pydantic = "latest"

[cerebrium.dependencies.conda]

[cerebrium.dependencies.apt]
