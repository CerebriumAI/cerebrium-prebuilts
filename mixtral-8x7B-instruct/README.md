# Mixtral 8x7B instruct

## Description

The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks tested.

## Usage

Deploy this model using the command: cerebrium deploy

- Parameters

| prompt | str | The input text prompt for the mixtral | *None specifiedâ€”it's a required field* |